{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "977712c9",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "## Overview\n",
    "\n",
    "A typical RAG application has two main components:\n",
    "\n",
    "**Indexing**: a pipeline for ingesting data from a source and indexing it. _This usually happens offline._\n",
    "\n",
    "**Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n",
    "The most common full sequence from raw data to answer looks like:\n",
    "\n",
    "### Indexing\n",
    "\n",
    "1. **Load**: First we need to load our data. This is done with [Document Loaders](https://python.langchain.com/docs/concepts/document_loaders/).\n",
    "2. **Split**: [Text splitters](https://python.langchain.com/docs/concepts/text_splitters/) break large `Documents` into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "3. **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a [VectorStore](https://python.langchain.com/docs/concepts/vectorstores/) and [Embeddings](https://python.langchain.com/docs/concepts/embedding_models/) model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403dfd8e",
   "metadata": {},
   "source": [
    "Side note:\n",
    "\n",
    "The context window (or “context length”) of a large language model (LLM) is the amount of text, in tokens, that the model can consider or “remember” at any one time. A larger context window enables an AI model to process longer inputs and incorporate a greater amount of information into each output.\n",
    "\n",
    "> https://codingscape.com/blog/llms-with-largest-context-windows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b07786",
   "metadata": {},
   "source": [
    "![rag_indexing_phase](../assets/images/rag_indexing_phase.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61aa870",
   "metadata": {},
   "source": [
    "### Retrieval and generation\n",
    "\n",
    "4. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](https://python.langchain.com/docs/concepts/retrievers/).\n",
    "5. **Generate**: A [ChatModel](https://python.langchain.com/docs/concepts/chat_models/) / [LLM](https://python.langchain.com/docs/concepts/text_llms/) produces an answer using a prompt that includes both the question with the retrieved data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab271f7d",
   "metadata": {},
   "source": [
    "![](../assets/images/rag_phase_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72c8de6",
   "metadata": {},
   "source": [
    "![rag_p2](../assets/images/rag_p2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1acb633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e1f5d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "\n",
    "# https://python.langchain.com/docs/integrations/text_embedding/sentence_transformers/\n",
    "\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_func = HuggingFaceEmbeddings(model_name=EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"rag_langchain\",\n",
    "    embedding_function=embedding_func,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec6fcbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(\"../data/LeaveNoContextBehind.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a17b70a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c1a24dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f9e4bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-11T01:01:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-11T01:01:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/LeaveNoContextBehind.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='Preprint. Under review.\\nLeave No Context Behind:\\nEfficient Infinite Context Transformers with Infini-attention\\nTsendsuren Munkhdalai, Manaal Faruqui and Siddharth Gopal\\nGoogle\\ntsendsuren@google.com\\nAbstract\\nThis work introduces an efficient method to scale Transformer-based Large\\nLanguage Models (LLMs) to infinitely long inputs with bounded memory\\nand computation. A key component in our proposed approach is a new at-\\ntention technique dubbed Infini-attention. The Infini-attention incorporates\\na compressive memory into the vanilla attention mechanism and builds\\nin both masked local attention and long-term linear attention mechanisms\\nin a single Transformer block. We demonstrate the effectiveness of our\\napproach on long-context language modeling benchmarks, 1M sequence\\nlength passkey context block retrieval and 500K length book summarization\\ntasks with 1B and 8B LLMs. Our approach introduces minimal bounded\\nmemory parameters and enables fast streaming inference for LLMs.\\n1 Introduction\\nMemory serves as a cornerstone of intelligence, as it enables efficient computations tailored\\nto specific contexts. However, Transformers (Vaswani et al., 2017) and Transformer-based\\nLLMs (Brown et al., 2020; Touvron et al., 2023; Anil et al., 2023; Groeneveld et al., 2024) have\\na constrained context-dependent memory, due to the nature of the attention mechanism.\\nUpdate\\nVV\\nConcat Concat\\nQ VQ VQs {KV}s\\nCompressive memory & \\nLinear attention\\nCausal scaled dot-product \\nattention & PE\\nLinear \\nprojection\\n{KV}s-1\\nRetrieve\\nFigure 1: Infini-attention has an addi-\\ntional compressive memory with linear\\nattention for processing infinitely long\\ncontexts. {KV}s−1 and {KV}s are atten-\\ntion key and values for current and previ-\\nous input segments, respectively and Qs\\nthe attention queries. PE denotes position\\nembeddings.\\nThe attention mechanism in Transformers ex-\\nhibits quadratic complexity in both memory\\nfootprint and computation time. For example,\\nthe attention Key-Value (KV) states have 3TB\\nmemory footprint for a 500B model with batch\\nsize 512 and context length 2048 (Pope et al.,\\n2023). Indeed, scaling LLMs to longer sequences\\n(i.e. 1M tokens) is challenging with the standard\\nTransformer architectures and serving longer\\nand longer context models becomes costly finan-\\ncially.\\nCompressive memory systems promise to be\\nmore scalable and efficient than the attention\\nmechanism for extremely long sequences (Kan-\\nerva, 1988; Munkhdalai et al., 2019). Instead\\nof using an array that grows with the input se-\\nquence length, a compressive memory primarily\\nmaintains a fixed number of parameters to store\\nand recall information with a bounded storage\\nand computation costs. In the compressive mem-\\nory, new information is added to the memory\\nby changing its parameters with an objective\\nthat this information can be recovered back later\\non. However, the LLMs in their current state\\nhave yet to see an effective, practical compres-\\nsive memory technique that balances simplicity along with quality.\\n1\\narXiv:2404.07143v1  [cs.CL]  10 Apr 2024')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f614dabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprint. Under review.\n",
      "Leave No Context Behind:\n",
      "Efficient Infinite Context Transformers with Infini-attention\n",
      "Tsendsuren Munkhdalai, Manaal Faruqui and Siddharth Gopal\n",
      "Google\n",
      "tsendsuren@google.com\n",
      "Abstract\n",
      "This work introduces an efficient method to scale Transformer-based Large\n",
      "Language Models (LLMs) to infinitely long inputs with bounded memory\n",
      "and computation. A key component in our proposed approach is a new at-\n",
      "tention technique dubbed Infini-attention. The Infini-attention incorporates\n",
      "a compressive memory into the vanilla attention mechanism and builds\n",
      "in both masked local attention and long-term linear attention mechanisms\n",
      "in a single Transformer block. We demonstrate the effectiveness of our\n",
      "approach on long-context language modeling benchmarks, 1M sequence\n",
      "length passkey context block retrieval and 500K length book summarization\n",
      "tasks with 1B and 8B LLMs. Our approach introduces minimal bounded\n",
      "memory parameters and enables fast streaming inference for LLMs.\n",
      "1 Introduction\n",
      "Memory serves as a cornerstone of intelligence, as it enables efficient computations tailored\n",
      "to specific contexts. However, Transformers (Vaswani et al., 2017) and Transformer-based\n",
      "LLMs (Brown et al., 2020; Touvron et al., 2023; Anil et al., 2023; Groeneveld et al., 2024) have\n",
      "a constrained context-dependent memory, due to the nature of the attention mechanism.\n",
      "Update\n",
      "VV\n",
      "Concat Concat\n",
      "Q VQ VQs {KV}s\n",
      "Compressive memory & \n",
      "Linear attention\n",
      "Causal scaled dot-product \n",
      "attention & PE\n",
      "Linear \n",
      "projection\n",
      "{KV}s-1\n",
      "Retrieve\n",
      "Figure 1: Infini-attention has an addi-\n",
      "tional compressive memory with linear\n",
      "attention for processing infinitely long\n",
      "contexts. {KV}s−1 and {KV}s are atten-\n",
      "tion key and values for current and previ-\n",
      "ous input segments, respectively and Qs\n",
      "the attention queries. PE denotes position\n",
      "embeddings.\n",
      "The attention mechanism in Transformers ex-\n",
      "hibits quadratic complexity in both memory\n",
      "footprint and computation time. For example,\n",
      "the attention Key-Value (KV) states have 3TB\n",
      "memory footprint for a 500B model with batch\n",
      "size 512 and context length 2048 (Pope et al.,\n",
      "2023). Indeed, scaling LLMs to longer sequences\n",
      "(i.e. 1M tokens) is challenging with the standard\n",
      "Transformer architectures and serving longer\n",
      "and longer context models becomes costly finan-\n",
      "cially.\n",
      "Compressive memory systems promise to be\n",
      "more scalable and efficient than the attention\n",
      "mechanism for extremely long sequences (Kan-\n",
      "erva, 1988; Munkhdalai et al., 2019). Instead\n",
      "of using an array that grows with the input se-\n",
      "quence length, a compressive memory primarily\n",
      "maintains a fixed number of parameters to store\n",
      "and recall information with a bounded storage\n",
      "and computation costs. In the compressive mem-\n",
      "ory, new information is added to the memory\n",
      "by changing its parameters with an objective\n",
      "that this information can be recovered back later\n",
      "on. However, the LLMs in their current state\n",
      "have yet to see an effective, practical compres-\n",
      "sive memory technique that balances simplicity along with quality.\n",
      "1\n",
      "arXiv:2404.07143v1  [cs.CL]  10 Apr 2024\n"
     ]
    }
   ],
   "source": [
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "715ac4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34d18e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "068fd511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f93be0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprint. Under review.\n",
      "Leave No Context Behind:\n",
      "Efficient Infinite Context Transformers with Infini-attention\n",
      "Tsendsuren Munkhdalai, Manaal Faruqui and Siddharth Gopal\n",
      "Google\n",
      "tsendsuren@google.com\n",
      "Abstract\n",
      "This work introduces an efficient method to scale Transformer-based Large\n",
      "Language Models (LLMs) to infinitely long inputs with bounded memory\n",
      "and computation. A key component in our proposed approach is a new at-\n",
      "tention technique dubbed Infini-attention. The Infini-attention incorporates\n",
      "a compressive memory into the vanilla attention mechanism and builds\n",
      "in both masked local attention and long-term linear attention mechanisms\n",
      "in a single Transformer block. We demonstrate the effectiveness of our\n",
      "approach on long-context language modeling benchmarks, 1M sequence\n",
      "length passkey context block retrieval and 500K length book summarization\n",
      "tasks with 1B and 8B LLMs. Our approach introduces minimal bounded\n",
      "memory parameters and enables fast streaming inference for LLMs.\n",
      "1 Introduction\n"
     ]
    }
   ],
   "source": [
    "print(all_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87987ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35012a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1185eeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NikhilSharma\\.conda\\envs\\llms2\\Lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f89e2332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: What is AI? \n",
      "Context: AI is ... \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(question=\"What is AI?\", context='AI is ...'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ccdda22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = {'question': 'What is AI?', 'context':'AI is ...'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6060bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: What is AI? \n",
      "Context: AI is ... \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(**c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d9e984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a Connection with the ChromaDB\n",
    "db_connection = Chroma(collection_name=\"rag_langchain\",persist_directory=\"./chroma_langchain_db\", embedding_function=embedding_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8228f998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n"
     ]
    }
   ],
   "source": [
    "# Converting CHROMA db_connection to Retriever Object\n",
    "retriever = db_connection.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(type(retriever))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eafdf9",
   "metadata": {},
   "source": [
    "##\n",
    "HW: Understand most common output parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5b123f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hii\n",
      "\n",
      "how are you...\n",
      "\n",
      "I am sleepy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# docs = ['hii', 'how are you...', 'I am sleepy']\n",
    "\n",
    "# print(\"\\n\\n\".join(doc for doc in docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "afaa9a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ccdca998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context from the retreiver -> list of documents and I need to stich/sew, so that we can pass on single string as context to the LLM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d9f5861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {'context':retriever | format_docs, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    |llm\n",
    "    |output_parser\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af9260c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This work introduces an efficient method called Infini-attention to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. Infini-attention incorporates a compressive memory into the vanilla attention mechanism, combining masked local attention and long-term linear attention. This allows it to reuse old KV attention states from previous segments, maintaining the entire context history efficiently.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = rag_chain.invoke(\"\"\"Please summarize Leave No Context Behind:\n",
    "                            Efficient Infinite Context Transformers with Infinite-attention\"\"\")\n",
    "\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b1f6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "497f9a3a",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5a3a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    # System Message Prompt Template\n",
    "    SystemMessage(content=\"\"\"You are a Helpful AI Bot.\n",
    "                  Given a context and question from user,\n",
    "                  you should answer based on the given context.\"\"\"),\n",
    "    # Human Message Prompt Template\n",
    "    HumanMessagePromptTemplate.from_template(\"\"\"Answer the question based on the given context.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer: \"\"\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d28c7524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This work introduces an efficient method called Infini-attention to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. Infini-attention incorporates a compressive memory into the vanilla attention mechanism, combining masked local attention and long-term linear attention. This allows it to reuse old KV attention states from previous segments, maintaining the entire context history efficiently."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "md(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "36e409aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Compressive memory maintains a fixed number of parameters to store and recall information, ensuring bounded storage and computation costs. New information is added by modifying these parameters with an objective for later recovery. Unlike memory arrays that grow with input sequence length, this approach offers computational efficiency.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"\"\"Please Explain Compressive Memory\"\"\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "986e7409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Compressive memory maintains a fixed number of parameters to store and recall information, ensuring bounded storage and computation costs. New information is added by modifying these parameters with an objective for later recovery. Unlike memory arrays that grow with input sequence length, this approach offers computational efficiency."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "md(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d226e8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
