{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a9bb8a5",
   "metadata": {},
   "source": [
    "![langchain_star_history](../assets/images/langchain_star_history.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ead46e",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "\n",
    "> LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4f13a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f982b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "gemini_llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n",
    "# gpt_llm = init_chat_model(\"gpt-3.5-turbo\", model_provider=\"openai\")\n",
    "# anthropic_llm = init_chat_model(\"anthropic....\", model_provider=\"anthropic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "323c3f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AI stands for **Artificial Intelligence**.\\n\\nAt its core, AI refers to the simulation of human intelligence in machines that are programmed to think and learn like humans. It\\'s about enabling computers to perform tasks that typically require human cognition, such as:\\n\\n1.  **Learning:** Acquiring information and rules for using the information.\\n2.  **Reasoning:** Using rules to reach approximate or definite conclusions.\\n3.  **Problem-solving:** Applying reasoning to solve specific problems.\\n4.  **Perception:** Understanding and interpreting the world through senses (like computer vision).\\n5.  **Language understanding:** Comprehending and generating human language (Natural Language Processing).\\n\\n**Key Concepts and Subfields of AI:**\\n\\n*   **Machine Learning (ML):** This is the most common and powerful subset of AI today. ML systems learn from data without being explicitly programmed. Instead of a human writing rules for every possible scenario, the machine identifies patterns and makes predictions or decisions based on the data it has been trained on.\\n    *   **Supervised Learning:** Learning from labeled data (e.g., showing a system thousands of cat pictures labeled \"cat\" to teach it to identify cats).\\n    *   **Unsupervised Learning:** Finding patterns in unlabeled data (e.g., grouping similar customer behaviors without prior definitions).\\n    *   **Reinforcement Learning:** Learning through trial and error, receiving rewards or penalties for actions (e.g., teaching a computer to play chess by rewarding good moves).\\n\\n*   **Deep Learning (DL):** A subfield of Machine Learning that uses artificial neural networks with multiple layers (hence \"deep\") to learn complex patterns from large amounts of data. Deep learning is behind many breakthroughs in areas like image recognition, natural language processing, and speech recognition.\\n\\n*   **Natural Language Processing (NLP):** Enables computers to understand, interpret, and generate human language. Examples include translation services, chatbots, spam filters, and sentiment analysis.\\n\\n*   **Computer Vision:** Allows computers to \"see\" and interpret visual information from images and videos. This powers facial recognition, self-driving cars, medical image analysis, and augmented reality.\\n\\n*   **Robotics:** While not exclusively AI, AI is often the \"brain\" that allows robots to perceive their environment, navigate, make decisions, and interact with the world autonomously.\\n\\n**Types of AI (Theoretical Spectrum):**\\n\\n1.  **Narrow AI (ANI) / Weak AI:** This is *all the AI that exists today*. It\\'s designed and trained for a specific task (e.g., playing chess, recommending movies, facial recognition, language translation). It can perform its specific task exceptionally well, often surpassing human ability, but it doesn\\'t possess general human-like intelligence.\\n\\n2.  **General AI (AGI) / Strong AI:** This is hypothetical AI that would possess human-level cognitive abilities across a wide range of tasks, capable of understanding, learning, and applying its intelligence to any intellectual task that a human being can. We are not yet close to achieving AGI.\\n\\n3.  **Superintelligence (ASI):** Also hypothetical, ASI would surpass human intelligence in every field, including creativity, general wisdom, and problem-solving. This is largely the realm of science fiction for now.\\n\\n**Where do you encounter AI?**\\n\\n*   **Smartphones:** Voice assistants (Siri, Google Assistant), facial recognition unlock, predictive text, photo filters.\\n*   **Streaming Services:** Personalized recommendations (Netflix, Spotify).\\n*   **E-commerce:** Product recommendations (Amazon), fraud detection.\\n*   **Healthcare:** Medical imaging analysis, drug discovery, personalized treatment plans.\\n*   **Finance:** Fraud detection, algorithmic trading, credit scoring.\\n*   **Transportation:** Self-driving cars, traffic optimization.\\n*   **Customer Service:** Chatbots, automated call routing.\\n*   **Search Engines:** Ranking results, understanding queries.\\n\\nIn essence, AI is a rapidly evolving field focused on creating intelligent machines that can learn, adapt, and perform tasks that traditionally require human intelligence, with the potential to profoundly impact nearly every aspect of our lives.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--1a39eb17-005b-4afb-a99e-f638b84f0883-0', usage_metadata={'input_tokens': 5, 'output_tokens': 864, 'total_tokens': 2099, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1230}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_llm.invoke(\"What is AI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1b8a29",
   "metadata": {},
   "source": [
    "![langchain_architecture.png](..\\assets\\images\\langchain_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf91b3",
   "metadata": {},
   "source": [
    "### LangChain use cases\n",
    "\n",
    "Applications made with LangChain provide great utility for a variety of use cases, from straightforward question-answering and text generation tasks to more complex solutions that use an LLM as a “reasoning engine.”\n",
    "\n",
    "- **Chatbots:** Chatbots are among the most intuitive uses of LLMs. LangChain can be used to provide proper context for the specific use of a chatbot, and to integrate chatbots into existing communication channels and workflows with their own APIs.\n",
    "    \n",
    "- **Summarization:** Language models can be tasked with summarizing many types of text, from breaking down complex academic articles and transcripts to providing a digest of incoming emails.\n",
    "    \n",
    "- **Question answering**: Using specific documents or specialized knowledge bases (like Wolfram, arXiv or PubMed), LLMs can retrieve relevant information from storage and articulate helpful answers). If fine-tuned or properly prompted, some LLMs can answer many questions even without external information.\n",
    "    \n",
    "- **Data augmentation:** LLMs can be used to generate synthetic data for use in machine learning. For example, an LLM can be trained to generate additional data samples that closely resemble the data points in a training dataset.\n",
    "    \n",
    "- **Virtual agents**: Integrated with the right workflows, LangChain’s Agent modules can use an LLM to autonomously determine next steps and take action using robotic process automation (RPA).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79332e7",
   "metadata": {},
   "source": [
    "![langchain_components.png](../assets/images/langchain_components.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_mesage -> comp1 -> cpmp2 -> comp3 -> final-response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7659ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Write an awesome article on 'special/magic methods in python' and the targted audience is 'Expereinced Python Developers'\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic = \"special/magic methods in python\"\n",
    "audience = \"Expereinced Python Developers\"\n",
    "\n",
    "prompt = f\"Write an awesome article on '{topic}' and the targted audience is '{audience}'\"\n",
    "\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d1a9df",
   "metadata": {},
   "source": [
    "## ChatModels\n",
    "\n",
    "https://python.langchain.com/docs/integrations/chat/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "result = llm.invoke(\"What is the Capital of India?\")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Different models\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "#LangChain's Chat Models Docs\n",
    "# https://python.langchain.com/docs/integrations/chat/\n",
    "\n",
    "# Setup environment variables and messages\n",
    "load_dotenv()\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful math assistant.\"),\n",
    "    HumanMessage(content=\"What is the square root of 49?\"),\n",
    "]\n",
    "\n",
    "\n",
    "# ---- LangChain OpenAI Chat Model Example ----\n",
    "# Create a ChatOpenAI model\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Invoke the model with messages\n",
    "result = model.invoke(messages)\n",
    "print(f\"Answer from OpenAI: {result.content}\")\n",
    "\n",
    "\n",
    "# ---- Anthropic Chat Model Example ----\n",
    "\n",
    "# Create a Anthropic model\n",
    "# Anthropic models: https://docs.anthropic.com/en/docs/models-overview\n",
    "# model = ChatAnthropic(model=\"claude-3-opus-20240229\")\n",
    "\n",
    "# result = model.invoke(messages)\n",
    "# print(f\"Answer from Anthropic: {result.content}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---- Google Chat Model Example ----\n",
    "\n",
    "# https://console.cloud.google.com/gen-app-builder/engines\n",
    "# https://ai.google.dev/gemini-api/docs/models/gemini\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "result = model.invoke(messages)\n",
    "print(f\"Answer from Google: {result.content}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2397e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK: use ollama in your python code, without langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "# model_name = \"llama3.2:1b-instruct-fp16\"\n",
    "model_name = \"phi3\"\n",
    "llm = ChatOllama(temperature=0.0, model=model_name)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Machine learning is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access and interpret data to perform automated tasks effectively, such as identifying patterns or making decisions with minimal human intervention. Machine learning algorithms use statistical techniques to give computers a \"learning capability,\" allowing them to become more accurate in predicting outcomes without being explicitly programmed for specific rules.\\n\\nThere are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning. Supervised learning involves using labeled data (data with known answers) to train models that can make predictions or decisions based on new inputs. Unsupervised learning uses unlabeled data without predefined outcomes for the model to identify patterns and relationships within the dataset itself. Reinforcement learning is a type of machine learning where an agent learns how to behave in an environment by performing certain actions and observing the rewards or penalties that follow, aiming to maximize its reward over time through trial-and-error experiences.\\n\\nMachine learning has numerous applications across various industries such as healthcare (predictive analytics for disease diagnosis), finance (fraud detection systems), retail (recommendation engines and customer segmentation), transportation, energy management, natural language processing, computer vision, robotic process automation, among others. As the technology continues to evolve rapidly with advancements in computing power, data availability, and algorithm development, machine learning is expected to play an increasingly significant role across various aspects of society and industry.', additional_kwargs={}, response_metadata={'model': 'phi3', 'created_at': '2025-09-22T07:05:47.4116877Z', 'done': True, 'done_reason': 'stop', 'total_duration': 100308615800, 'load_duration': 17732223800, 'prompt_eval_count': 13, 'prompt_eval_duration': 2925200400, 'eval_count': 350, 'eval_duration': 79149896400, 'model_name': 'phi3'}, id='run--0b169227-bec8-4fab-9949-1494190ae117-0', usage_metadata={'input_tokens': 13, 'output_tokens': 350, 'total_tokens': 363})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is Machine Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4423f8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c99f95fc",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3d0182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sad\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/@smrati.katiyar/langchain-prompts-prompttemplate-and-chatprompttemplate-a1d76c02c18c\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "ChatPromptTemplate,\n",
    "SystemMessagePromptTemplate,\n",
    "AIMessagePromptTemplate,\n",
    "HumanMessagePromptTemplate,)\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "template=\"You are a helpful assistant who can give {category} for given input. Only give {category} no other text, use as little words as possible while responding, better give reply only in one word.\"\n",
    "\n",
    "\n",
    "\n",
    "system_message_prompt=SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "human_template=\"{text}\"\n",
    "\n",
    "\n",
    "human_message_prompt=HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt=ChatPromptTemplate.from_messages([system_message_prompt,human_message_prompt])\n",
    "\n",
    "chat = ChatOllama(model=\"llama3.2:1b\",temperature=0,)\n",
    "resp = chat.invoke(chat_prompt.format_prompt(category=\"antonyms\", text=\"Happy\").to_messages())\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdebd85d",
   "metadata": {},
   "source": [
    "### Implement the below article\n",
    "\n",
    "> https://medium.com/@shravankoninti/different-prompt-templates-using-langchain-3c8dd3aca3be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2021cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afaff6f4",
   "metadata": {},
   "source": [
    "## Basic Project\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6479ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install -qU \\\n",
    "#   langchain-core==0.3.33 \\\n",
    "#   langchain-openai==0.3.3 \\\n",
    "#   langchain-community==0.3.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d77bdbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\", temperature=0)\n",
    "\n",
    "creative_llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\", temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90552bd",
   "metadata": {},
   "source": [
    "We will be taking an `article` _draft_ and using LangChain to generate various useful items around this article. We'll be creating:\n",
    "\n",
    "1. An article title\n",
    "2. An article description\n",
    "3. Editor advice where we will insert an additional paragraph in the article\n",
    "4. A thumbnail / hero image for our article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89a24212",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "\\\n",
    "We believe AI's short—to mid-term future belongs to agents and that the long-term future of *AGI* may evolve from agentic systems. Our definition of agents covers any neuro-symbolic system in which we merge neural AI (such as an LLM) with semi-traditional software.\n",
    "\n",
    "With agents, we allow LLMs to integrate with code — allowing AI to search the web, perform math, and essentially integrate into anything we can build with code. It should be clear the scope of use cases is phenomenal where AI can integrate with the broader world of software.\n",
    "\n",
    "In this introduction to AI agents, we will cover the essential concepts that make them what they are and why that will make them the core of real-world AI in the years to come.\n",
    "\n",
    "---\n",
    "\n",
    "## Neuro-Symbolic Systems\n",
    "\n",
    "Neuro-symbolic systems consist of both neural and symbolic computation, where:\n",
    "\n",
    "- Neural refers to LLMs, embedding models, or other neural network-based models.\n",
    "- Symbolic refers to logic containing symbolic logic, such as code.\n",
    "\n",
    "Both neural and symbolic AI originate from the early philosophical approaches to AI: connectionism (now neural) and symbolism. Symbolic AI is the more traditional AI. Diehard symbolists believed they could achieve true AGI via written rules, ontologies, and other logical functions.\n",
    "\n",
    "The other camp were the connectionists. Connectionism emerged in 1943 with a theoretical neural circuit but truly kicked off with Rosenblatt's perceptron paper in 1958 [1][2]. Both of these approaches to AI are fascinating but deserve more time than we can give them here, so we will leave further exploration of these concepts for a future chapter.\n",
    "\n",
    "Most important to us is understanding where symbolic logic outperforms neural-based compute and vice-versa.\n",
    "\n",
    "| Neural | Symbolic |\n",
    "| --- | --- |\n",
    "| Flexible, learned logic that can cover a huge range of potential scenarios. | Mostly hand-written rules which can be very granular and fine-tuned but hard to scale. |\n",
    "| Hard to interpret why a neural system does what it does. Very difficult or even impossible to predict behavior. | Rules are written and can be understood. When unsure why a particular ouput was produced we can look at the rules / logic to understand. |\n",
    "| Requires huge amount of data and compute to train state-of-the-art neural models, making it hard to add new abilities or update with new information. | Code is relatively cheap to write, it can be updated with new features easily, and latest information can often be added often instantaneously. |\n",
    "| When trained on broad datasets can often lack performance when exposed to unique scenarios that are not well represented in the training data. | Easily customized to unique scenarios. |\n",
    "| Struggles with complex computations such as mathematical operations. | Perform complex computations very quickly and accurately. |\n",
    "\n",
    "Pure neural architectures struggle with many seemingly simple tasks. For example, an LLM *cannot* provide an accurate answer if we ask it for today's date.\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is commonly used to provide LLMs with up-to-date knowledge on a particular subject or access to proprietary knowledge.\n",
    "\n",
    "### Giving LLMs Superpowers\n",
    "\n",
    "By 2020, it was becoming clear that neural AI systems could not perform tasks symbolic systems typically excelled in, such as arithmetic, accessing structured DB data, or making API calls. These tasks require discrete input parameters that allow us to process them reliably according to strict written logic.\n",
    "\n",
    "In 2022, researchers at AI21 developed Jurassic-X, an LLM-based \"neuro-symbolic architecture.\" Neuro-symbolic refers to merging the \"neural computation\" of large language models (LLMs) with more traditional (i.e. symbolic) computation of code.\n",
    "\n",
    "Jurassic-X used the Modular Reasoning, Knowledge, and Language (MRKL) system [3]. The researchers developed MRKL to solve the limitations of LLMs, namely:\n",
    "\n",
    "- Lack of up-to-date knowledge, whether that is the latest in AI or something as simple as today's date.\n",
    "- Lack of proprietary knowledge, such as internal company docs or your calendar bookings.\n",
    "- Lack of reasoning, i.e. the inability to perform operations that traditional software is good at, like running complex mathematical operations.\n",
    "- Lack of ability to generalize. Back in 2022, most LLMs had to be fine-tuned to perform well in a specific domain. This problem is still present today but far less prominent as the SotA models generalize much better and, in the case of MRKL, are able to use tools relatively well (although we could certainly take the MRKL solution to improve tool use performance even today).\n",
    "\n",
    "MRKL represents one of the earliest forms of what we would now call an agent; it is an LLM (neural computation) paired with executable code (symbolic computation).\n",
    "\n",
    "## ReAct and Tools\n",
    "\n",
    "There is a misconception in the broader industry that an AI agent is an LLM contained within some looping logic that can generate inputs for and execute code functions. This definition of agents originates from the huge popularity of the ReAct agent framework and the adoption of a similar structure with function/tool calling by LLM providers such as OpenAI, Anthropic, and Ollama.\n",
    "\n",
    "![ReAct agent flow with the Reasoning-Action loop [4]. When the action chosen specifies to use a normal tool, the tool is used and the observation returned for another iteration through the Reasoning-Action loop. To return a final answer to the user the LLM must choose action \"answer\" and provide the natural language response, finishing the loop.](/images/posts/ai-agents/ai-agents-00.png)\n",
    "\n",
    "<small>ReAct agent flow with the Reasoning-Action loop [4]. When the action chosen specifies to use a normal tool, the tool is used and the observation returned for another iteration through the Reasoning-Action loop. To return a final answer to the user the LLM must choose action \"answer\" and provide the natural language response, finishing the loop.</small>\n",
    "\n",
    "Our \"neuro-symbolic\" definition is much broader but certainly does include ReAct agents and LLMs paired with tools. This agent type is the most common for now, so it's worth understanding the basic concept behind it.\n",
    "\n",
    "The **Re**ason **Act**ion (ReAct) method encourages LLMs to generate iterative *reasoning* and *action* steps. During *reasoning,* the LLM describes what steps are to be taken to answer the user's query. Then, the LLM generates an *action,* which we parse into an input to some executable code, which we typically describe as a tool/function call.\n",
    "\n",
    "![ReAct method. Each iteration includes a Reasoning step followed by an Action (tool call) step. The Observation is the output from the previous tool call. During the final iteration the agent calls the answer tool, meaning we generate the final answer for the user.](/images/posts/ai-agents/ai-agents-01.png)\n",
    "\n",
    "<small>ReAct method. Each iteration includes a Reasoning step followed by an Action (tool call) step. The Observation is the output from the previous tool call. During the final iteration the agent calls the answer tool, meaning we generate the final answer for the user.</small>\n",
    "\n",
    "Following the reason and action steps, our action tool call returns an observation. The logic returns the observation to the LLM, which is then used to generate subsequent reasoning and action steps.\n",
    "\n",
    "The ReAct loop continues until the LLM has enough information to answer the original input. Once the LLM reaches this state, it calls a special *answer* action with the generated answer for the user.\n",
    "\n",
    "## Not only LLMs and Tool Calls\n",
    "\n",
    "LLMs paired with tool calling are powerful but far from the only approach to building agents. Using the definition of neuro-symbolic, we cover architectures such as:\n",
    "\n",
    "- Multi-agent workflows that involve multiple LLM-tool (or other agent structure) combinations.\n",
    "- More deterministic workflows where we may have set neural model-tool paths that may fork or merge as the use case requires.\n",
    "- Embedding models that can detect user intents and decide tool-use or LLM selection-based selection in vector space.\n",
    "\n",
    "These are just a few high-level examples of alternative agent structures. Far from being designed for niche use cases, we find these alternative options to frequently perform better than the more common ReAct or Tool agents. We will cover all of these examples and more in future chapters.\n",
    "\n",
    "---\n",
    "\n",
    "Agents are fundamental to the future of AI, but that doesn't mean we should expect that future to come from agents in their most popular form today. ReAct and Tool agents are great and handle many simple use cases well, but the scope of agents is much broader, and we believe thinking beyond ReAct and Tools is key to building future AI.\n",
    "\n",
    "---\n",
    "\n",
    "You can sign up for the [Aurelio AI newsletter](https://b0fcw9ec53w.typeform.com/to/w2BDHVK7) to stay updated on future releases in our comprehensive course on agents.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "[1] The curious case of Connectionism (2019) [https://www.degruyter.com/document/doi/10.1515/opphil-2019-0018/html](https://www.degruyter.com/document/doi/10.1515/opphil-2019-0018/html)\n",
    "\n",
    "[2] F. Rosenblatt, [The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf) (1958), Psychological Review\n",
    "\n",
    "[3] E. Karpas et al. [MRKL Systems: A Modular, Neuro-Symbolic Architecture That Combines Large Language Models, External Knowledge Sources and Discrete Reasoning](https://arxiv.org/abs/2205.00445) (2022), AI21 Labs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a34eaf0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWe believe AI's short—to mid-term future belongs to agents and that the long-te\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article[:80]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf2afd",
   "metadata": {},
   "source": [
    "LangChain comes with several prompt classes and methods for organizing or constructing our prompts.\n",
    "\n",
    "Prompts for chat agents are at a minimum broken up into three components, those are:\n",
    "\n",
    "* System prompt: this provides the instructions to our LLM on how it must behave, what it's objective is, etc.\n",
    "\n",
    "* User prompt: this is a user written input.\n",
    "\n",
    "* AI prompt: this is the AI generated output. When representing a conversation, previous generations will be inserted back into the next prompt and become part of the broader _chat history_.\n",
    "\n",
    "```\n",
    "You are a helpful AI assistant, you will do XYZ.    | SYSTEM PROMPT\n",
    "\n",
    "User: Hi, what is the capital of Australia?         | USER PROMPT\n",
    "AI: It is Canberra                                  | AI PROMPT\n",
    "User: When is the best time to visit?               | USER PROMPT\n",
    "```\n",
    "\n",
    "LangChain provides us with _templates_ for each of these prompt types. By using templates we can insert different inputs to the template, modifying the prompt based on the provided inputs.\n",
    "\n",
    "Let's initialize our system and user prompt first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2b7527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# Defining the system prompt (how the AI should act)\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI assistant that helps generate article titles.\"\n",
    ")\n",
    "\n",
    "# the user prompt is provided by the user, in this case however the only dynamic\n",
    "# input is the article\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a name for a article.\n",
    "The article is here for you to examine {article}\n",
    "\n",
    "The name should be based of the context of the article.\n",
    "Be creative, but make sure the names are clear, catchy,\n",
    "and relevant to the theme of the article.\n",
    "\n",
    "Only output the article name, no other explanation or\n",
    "text can be provided.\"\"\",\n",
    "    input_variables=[\"article\"]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2962b9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content=\"You are tasked with creating a name for a article.\\nThe article is here for you to examine \\nWe believe AI's short—to mid-term future belongs to agents and that the long-term future of *AGI* may evolve from agentic systems. Our definition of agents covers any neuro-symbolic system in which w\\n\\nThe name should be based of the context of the article.\\nBe creative, but make sure the names are clear, catchy,\\nand relevant to the theme of the article.\\n\\nOnly output the article name, no other explanation or\\ntext can be provided.\", additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt.format(article=article[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784254a1",
   "metadata": {},
   "source": [
    "We have our system and user prompts, we can merge both into our full chat prompt using the `ChatPromptTemplate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a01be28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b6316a",
   "metadata": {},
   "source": [
    "By default, the `ChatPromptTemplate` will read the `input_variables` from each of the prompt templates inserted and allow us to use those input variables when formatting the full chat prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5569d1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an AI assistant that helps generate article titles.\n",
      "Human: You are tasked with creating a name for a article.\n",
      "The article is here for you to examine \n",
      "We believe AI's short—to mid-term future belongs to agents and that the long-term future of *AGI* may evolve from agentic systems. Our definition of agents covers any neuro-symbolic system in which w\n",
      "\n",
      "The name should be based of the context of the article.\n",
      "Be creative, but make sure the names are clear, catchy,\n",
      "and relevant to the theme of the article.\n",
      "\n",
      "Only output the article name, no other explanation or\n",
      "text can be provided.\n"
     ]
    }
   ],
   "source": [
    "print(first_prompt.format(article=article[:200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209d454",
   "metadata": {},
   "source": [
    "`ChatPromptTemplate` also prefixes each individual message with it's role, ie `System:`, `Human:`, or `AI:`.\n",
    "\n",
    "We can chain together our `first_prompt` template and the `llm` object we defined earlier to create a simple LLM chain. This chain will perform the steps **prompt formatting > llm generation > get output**.\n",
    "\n",
    "We'll be using **L**ang**C**hain **E**xpression **L**anguage (LCEL) to construct our chain. This syntax can look a little strange but we will cover it in detail later....\n",
    "For now, all we need to know is that we define our inputs with the first dictionary segment (ie `{\"article\": lambda x: x[\"article\"]}`) and then we use the pipe operator (`|`) to say that the output from the left of the pipe will be fed into the input to the right of the pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29690a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain_one = (\n",
    "#      input\n",
    "#     | c1 \n",
    "#     | c2\n",
    "#     | c3\n",
    "    \n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13d688b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here is some text...'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'article': \"here is some text...\"}\n",
    "fun = lambda x: x[\"article\"]\n",
    "fun(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2752815",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_one = (\n",
    "    {\"article\": lambda x: x[\"article\"]}\n",
    "    | first_prompt\n",
    "    | creative_llm\n",
    "    | {\"article_title\": lambda x: x.content}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414699c2",
   "metadata": {},
   "source": [
    "1. Takes an input with an \"article\" key\n",
    "2. Extracts just the article content \n",
    "3. Passes it through a prompt template (`first_prompt`)\n",
    "4. Sends it to a creative LLM\n",
    "5. Takes the LLM's response content and maps it to \"article_title\"\n",
    "\n",
    "**LCEL (LangChain Expression Language)** is LangChain's declarative syntax for building AI chains. Key features:\n",
    "\n",
    "- **Pipe operator (`|`)**: Chains components together sequentially\n",
    "- **Dict syntax (`{}`)**: Maps inputs/outputs between steps\n",
    "- **Lambda functions**: Transform data inline\n",
    "- **Composable**: Easy to combine and modify chains\n",
    "\n",
    "The chain essentially transforms an article into a title using an LLM, with each `|` passing output from one step as input to the next.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6423267e",
   "metadata": {},
   "source": [
    "Our first chain creates the article title, note: we can run all of these individually..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68af0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_title_msg = chain_one.invoke({\"article\": article})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa78d05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI Agents: The Neuro-Symbolic Foundation of Future AI'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_title_msg[\"article_title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d00369",
   "metadata": {},
   "source": [
    "\n",
    "LangChain Framework\n",
    "    ChatModels\n",
    "        ChatOllama\n",
    "        ChatOpenAI\n",
    "        ChatAnthropic\n",
    "        init_chat_model\n",
    "    Prompt Templates\n",
    "        - Reuse..\n",
    "        - Dynamic\n",
    "        SystemMessagePromptTemplate\n",
    "        HumanMessagePromptTemplate\n",
    "        AIMessagePromptTemplate\n",
    "    \n",
    "    Chains\n",
    "        LCEL -> Langchain Expression Language\n",
    "        input\n",
    "        | comp1\n",
    "        | comp2\n",
    "        | ...\n",
    "\n",
    "Project\n",
    "    llm\n",
    "    creativellm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16035c2b",
   "metadata": {},
   "source": [
    "But we will actually chain this step with multiple other `LLMChain` steps. So, to continue, our next step is to summarize the article using both the `article` and newly generated `article_title` values, from which we will output a new `summary` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bb88042",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI assistant that helps summarize the articles.\"\n",
    ")\n",
    "\n",
    "second_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a description for\n",
    "the article. The article is here for you to examine:\n",
    "\n",
    "---\n",
    "\n",
    "{article}\n",
    "\n",
    "---\n",
    "\n",
    "Here is the article title '{article_title}'.\n",
    "\n",
    "Output the SEO friendly article description. Do not output\n",
    "anything other than the description.\"\"\",\n",
    "    input_variables=[\"article\", \"article_title\"]\n",
    ")\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_messages([\n",
    "    summarizer_system_prompt,\n",
    "    second_user_prompt\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64494e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_two = (\n",
    "    {'input_var1': 'val1',\n",
    "     'input_var2': 'val2'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_two = (\n",
    "    {\n",
    "        'article': lambda x: x['article'],\n",
    "        'article_title': lambda x: x[\"article_title\"]\n",
    "    }\n",
    "    | second_prompt\n",
    "    | llm\n",
    "    | {'summary': lambda x: x.content}     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b717575",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_description_msg = chain_two.invoke({\n",
    "    'article': article,\n",
    "    'article_title': article_title_msg[\"article_title\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9e197c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': 'Discover AI agents as the neuro-symbolic foundation of future AI. This article explains how agents merge large language models (LLMs) with traditional software, enabling advanced capabilities like web search, complex math, and real-world integration. Learn about the evolution of neuro-symbolic systems, from early architectures to ReAct agents, and why a broader understanding of agent types is key to unlocking AGI.'}\n"
     ]
    }
   ],
   "source": [
    "print(article_description_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a058fdb",
   "metadata": {},
   "source": [
    "The third step will consume our first `article` variable and provide several output fields, focusing on helping the user improve a part of their writing. As we are outputting multiple fields we can specify for the LLM to use structured outputs, keeping the generated fields aligned with our requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c69a6",
   "metadata": {},
   "source": [
    "### Structured Outputs\n",
    "\n",
    "1. https://python.langchain.com/docs/concepts/structured_outputs/\n",
    "2. https://medium.com/@juanc.olamendy/parsing-llm-structured-outputs-in-langchain-a-comprehensive-guide-f05ffa88261f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "901f8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "critique_system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI assistant that helps improve articles by providing the constructive feedbacks.\"\n",
    ")\n",
    "\n",
    "third_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a new paragraph for the\n",
    "article. The article is here for you to examine:\n",
    "\n",
    "---\n",
    "\n",
    "{article}\n",
    "\n",
    "---\n",
    "\n",
    "Choose one paragraph to review and edit. During your edit\n",
    "ensure you provide constructive feedback to the user so they\n",
    "can learn where to improve their own writing.\"\"\",\n",
    "    input_variables=[\"article\"]\n",
    ")\n",
    "\n",
    "# prompt template 3: creating a new paragraph for the article\n",
    "third_prompt = ChatPromptTemplate.from_messages([\n",
    "    critique_system_prompt,\n",
    "    third_user_prompt\n",
    "])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e9fed",
   "metadata": {},
   "source": [
    "We create a pydantic object describing the output format we need. This format description is then passed to our model using the `with_structured_output` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4e78abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Paragraph(BaseModel):\n",
    "    original_paragraph: str = Field(description=\"The original paragraph\")\n",
    "    edited_paragraph: str = Field(description=\"The improved edited paragraph\")\n",
    "    feedback: str = Field(description=(\n",
    "        \"Constructive feedback on the original paragraph\"\n",
    "    ))\n",
    "\n",
    "\n",
    "structured_llm = creative_llm.with_structured_output(Paragraph)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c88395",
   "metadata": {},
   "source": [
    "Now we put all of this together in another chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3a6bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_three = (\n",
    "    {'article': lambda x: x['article']}\n",
    "    | third_prompt\n",
    "    | structured_llm\n",
    "    | {\n",
    "        \"original_paragraph\": lambda x: x.original_paragraph,\n",
    "        \"edited_paragraph\": lambda x: x.edited_paragraph,\n",
    "        \"feedback\": lambda x: x.feedback\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82a1669b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_paragraph': \"Jurassic-X used the Modular Reasoning, Knowledge, and Language (MRKL) system [3]. The researchers developed MRKL to solve the limitations of LLMs, namely: - Lack of up-to-date knowledge, whether that is the latest in AI or something as simple as today's date. - Lack of proprietary knowledge, such as internal company docs or your calendar bookings. - Lack of reasoning, i.e. the inability to perform operations that traditional software is good at, like running complex mathematical operations. - Lack of ability to generalize. Back in 2022, most LLMs had to be fine-tuned to perform well in a specific domain. This problem is still present today but far less prominent as the SotA models generalize much better and, in the case of MRKL, are able to use tools relatively well (although we could certainly take the MRKL solution to improve tool use performance even today).\",\n",
       " 'edited_paragraph': \"Jurassic-X integrated the Modular Reasoning, Knowledge, and Language (MRKL) system [3]. Researchers developed MRKL to address several key limitations of LLMs at the time: their inability to access up-to-date information (like today's date or current events), their lack of proprietary knowledge (such as internal company documents), and their difficulty with complex reasoning and mathematical operations typically handled by traditional software. Furthermore, LLMs in 2022 often struggled with generalization, requiring extensive fine-tuning for specific domains. While state-of-the-art (SotA) models have since improved significantly in generalization and tool use, the MRKL system offered an early and effective solution to these challenges, enhancing LLMs' performance in diverse scenarios.\",\n",
       " 'feedback': \"The original paragraph was quite long and dense, making it a bit challenging to follow the multiple points being made. Here's a breakdown of the feedback:\\n\\n1.  **Conciseness and Flow:** The original's enumeration of limitations, while clear, could be integrated more smoothly into a narrative. The edited version condenses these points into more fluid sentences, improving the overall flow and readability.\\n\\n2.  **Acronym Clarity:** 'SotA' is an acronym common in the AI field, but it's good practice to spell it out ('state-of-the-art') upon its first mention in an article, especially for a broader audience. The edited paragraph clarifies this to ensure all readers understand.\\n\\n3.  **Sentence Structure and Pacing:** The final sentence in the original paragraph was particularly long and contained a parenthetical remark that disrupted its flow. Breaking down complex ideas into more manageable sentences or restructuring them can enhance comprehension. The edited version splits this into clearer, more focused statements, making the progression of ideas smoother.\\n\\n4.  **Temporal Context:** While the original paragraph highlighted the historical context of LLM limitations in 2022, the transition to current 'SotA models' and then back to the enduring relevance of MRKL felt a bit abrupt. The edited version aims to bridge these temporal shifts more effectively, maintaining a clearer progression of thought.\"}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = chain_three.invoke({\"article\": article})\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bd7672",
   "metadata": {},
   "source": [
    "Now we want this article to look appealing, so we need to grab an image based of our article! However the prompt for the article image `cannot be over 1000 letters` so this has to be short in case we want to add anything in such as `style` later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f54b5620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AI agent: Neuro-symbolic system. Glowing LLM brain merges with intricate code, dynamically interacting with the world via tools. Futuristic, conceptual digital art, vibrant, data streams, intricate connections.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--1f38bb74-5df9-4950-a71c-05a7b22883f4-0', usage_metadata={'input_tokens': 2161, 'output_tokens': 40, 'total_tokens': 4487, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 2286}})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "image_prompt = PromptTemplate(\n",
    "    input_variables=[\"article\"],\n",
    "    template=(\n",
    "        \"Generate a prompt with less then 500 characters to generate an image \"\n",
    "        \"based on the following article: {article}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "final_image_prompt = llm.invoke(image_prompt.invoke(article).text)\n",
    "\n",
    "final_image_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ab090b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0\\nPlease retry in 41.502945783s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-preview-image'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-preview-image'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-preview-image'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '41s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m client = genai.Client()\n\u001b[32m      8\u001b[39m prompt = final_image_prompt.content\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-2.5-flash-image-preview\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m response.candidates[\u001b[32m0\u001b[39m].content.parts:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m part.text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NikhilSharma\\.conda\\envs\\llms2\\Lib\\site-packages\\google\\genai\\models.py:5049\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5047\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5048\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5049\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5050\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[32m   5051\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5052\u001b[39m   logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is done.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   5053\u001b[39m   remaining_remote_calls_afc -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NikhilSharma\\.conda\\envs\\llms2\\Lib\\site-packages\\google\\genai\\models.py:4025\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4022\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   4023\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4025\u001b[39m response_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4026\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4027\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4029\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.vertexai:\n\u001b[32m   4030\u001b[39m   response_dict = _GenerateContentResponse_from_vertex(\n\u001b[32m   4031\u001b[39m       \u001b[38;5;28mself\u001b[39m._api_client, response_dict\n\u001b[32m   4032\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NikhilSharma\\.conda\\envs\\llms2\\Lib\\site-packages\\google\\genai\\_api_client.py:751\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    742\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    743\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    746\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    747\u001b[39m ) -> Union[BaseResponse, Any]:\n\u001b[32m    748\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m    749\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m    750\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m   json_response = response.json\n\u001b[32m    753\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NikhilSharma\\.conda\\envs\\llms2\\Lib\\site-packages\\google\\genai\\_api_client.py:680\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    673\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m    674\u001b[39m       method=http_request.method,\n\u001b[32m    675\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    678\u001b[39m       timeout=http_request.timeout,\n\u001b[32m    679\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m    682\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m    683\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NikhilSharma\\.conda\\envs\\llms2\\Lib\\site-packages\\google\\genai\\errors.py:101\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m     99\u001b[39m status_code = response.status_code\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    103\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0\\nPlease retry in 41.502945783s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-preview-image'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-preview-image'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-preview-image'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '41s'}]}}"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "prompt = final_image_prompt.content\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-image-preview\",\n",
    "    contents=[prompt],\n",
    ")\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "    if part.text is not None:\n",
    "        print(part.text)\n",
    "    elif part.inline_data is not None:\n",
    "        image = Image.open(BytesIO(part.inline_data.data))\n",
    "        image.save(\"generated_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d27ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
